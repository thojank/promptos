# Z-Image-Turbo Prompt Platform â€” Setup & NÃ¤chste Schritte

## Status: Phase 1 âœ… Abgeschlossen

Du hast jetzt das **GrundgerÃ¼st** deiner Applikation:

### âœ… Was wurde erstellt:

1. **[backend/models.py](backend/models.py)** â€” Pydantic-Schemas
   - `CharacterProfile` mit Anti-Bias-Physik-Deskriptoren
   - `SceneModule` (Setting, Lighting, Atmosphere, Composition)
   - `ActionModule` (Pose & Aktion)
   - `TextElementsModule` (Bilder-Text)
   - `ZImageTurboPrompt` (finale Kombination)
   - `Story` (narrative Sequenzen)
   - Validierungsfunktionen gegen Forbidden Words

2. **[backend/system_prompts.py](backend/system_prompts.py)** â€” Gemini-Instruktionen
   - `GEMINI_SYSTEM_PROMPT_TEXT_TO_JSON` â€” Text-Input â†’ Strukturiertes JSON
   - `GEMINI_SYSTEM_PROMPT_VISION` â€” Bild-Analyse â†’ Metadaten
   - `GEMINI_SYSTEM_PROMPT_JSON_TO_TEXT` â€” JSON â†’ 600-1000 Wort Prompt

3. **[backend/gemini_integration.py](backend/gemini_integration.py)** â€” API-Integration
   - `GeminiPromptGenerator` Klasse
   - `.text_to_json()` â€” Nutzer-Text in strukturiertes JSON
   - `.image_to_json()` â€” Bild-Upload â†’ JSON-Extraktion
   - `.json_to_prompt_text()` â€” JSON â†’ Finaler Prompt
   - `.generate_full_prompt()` â€” End-to-End (Text â†’ finaler Prompt)

4. **[backend/db_models.py](backend/db_models.py)** â€” Datenbank-Schema
   - `character_profiles` â€” Wiederverwendbare Character-Templates
   - `scene_modules` â€” Austauschbare Szenen
   - `text_elements` â€” Text-Objekte fÃ¼r Bilder
   - `stories` â€” Container fÃ¼r Prompt-Sequenzen
   - `prompts` â€” Finale Kombinationen
   - `story_versions` â€” Versionskontrolle

---

## ðŸš€ Phase 2: JETZT STARTEN (Deine nÃ¤chsten Schritte)

### Schritt 1: Umgebung einrichten

```bash
cd /Users/thorstenjankowski/n8n-compose/310126_prompt-platform

# Python 3.10+ erforderlich
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### Schritt 2: Gemini API-Key setzen

1.  Gehe zu Google AI Studio und erstelle einen API-Key.
2.  Hole dir die Datenbank-URL aus deinem Supabase-Projekt (Settings > Database > Connection string).
3.  Speichere beides in einer `.env`-Datei im Hauptverzeichnis:

```bash
# .env
GEMINI_API_KEY=dein_api_key_hier
DATABASE_URL=postgresql://postgres:[DEIN-PASSWORT]@[...].supabase.co:5432/postgres
```

### Schritt 3: Test der Gemini-Integration

```bash
cd backend
python gemini_integration.py
```

Dies sollte ein komplettes Beispiel durchlaufen:
- Input: Text-Beschreibung
- Output: Strukturiertes JSON + Finaler 600-1000 Wort Prompt

### Schritt 4: (Optional) Datenbank initialisieren

Falls du mit PostgreSQL arbeiten willst:

```bash
# PostgreSQL lokal starten (macOS mit Homebrew)
brew services start postgresql

# Datenbank erstellen
createdb zimage_turbo

# In Python initialieren:
python -c "from backend.db_models import init_database; init_database('postgresql://localhost/zimage_turbo')"
```

---

## ðŸŽ¯ Phase 3: API-Endpoints bauen (FastAPI)

Du brauchst jetzt einen REST-Server, um deine Gemini-Integration freizuschalten.

Erstelle **[backend/main.py](backend/main.py)** mit diesen Endpoints:

```python
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.responses import JSONResponse
from gemini_integration import GeminiPromptGenerator
from models import ZImageTurboPrompt, PromptAssemblyOutput
import json

app = FastAPI(title="Z-Image-Turbo Prompt Platform")
generator = GeminiPromptGenerator()

@app.post("/api/text-to-prompt")
async def text_to_prompt(text_input: str):
    """Text-Input â†’ Finaler Prompt"""
    try:
        output = generator.generate_full_prompt(text_input)
        return {
            "success": True,
            "prompt_text": output.full_prompt_text,
            "word_count": output.estimated_word_count,
            "validation_passed": output.forbidden_words_check,
            "json_structure": output.json_structure.model_dump()
        }
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

@app.post("/api/image-to-json")
async def image_to_json(file: UploadFile = File(...)):
    """Bild-Upload â†’ Metadaten-JSON"""
    try:
        # Speichere Bild temporÃ¤r
        import tempfile
        with tempfile.NamedTemporaryFile(delete=False, suffix=".jpg") as tmp:
            contents = await file.read()
            tmp.write(contents)
            tmp_path = tmp.name
        
        result = generator.image_to_json(tmp_path)
        return result.model_dump()
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

@app.get("/health")
async def health():
    return {"status": "ok"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
    uvicorn.run(app, host="0.0.0.0", port=8001)
```

Starten mit: `python backend/main.py`

---

## ðŸŽ¨ Phase 4: Frontend mit React/Next.js

Empfohlene Struktur:

```
frontend/
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ CharacterBuilder.tsx    # Nutzer wÃ¤hlt/erstellt Character
â”‚   â”œâ”€â”€ SceneSelector.tsx       # WÃ¤hlt Szene aus
â”‚   â”œâ”€â”€ PromptPreview.tsx       # Zeigt finalen Prompt
â”‚   â””â”€â”€ StoryPlanner.tsx        # Drag-and-Drop Story-Editor
â”œâ”€â”€ pages/
â”‚   â”œâ”€â”€ index.tsx               # Home
â”‚   â”œâ”€â”€ library.tsx             # Character/Scene Library
â”‚   â””â”€â”€ editor.tsx              # Story-Editor
â””â”€â”€ lib/
    â””â”€â”€ api.ts                  # API-Calls zu deinem Backend
```

**Erste UI:**
- Input-Feld fÃ¼r Text-Beschreibung
- Button "Generate Prompt"
- Output: JSON + finaler Prompt-Text
- Copy-to-Clipboard Button

---

## ðŸ’¡ WICHTIGE DESIGN-ENTSCHEIDUNGEN

### ModularitÃ¤t: Das Kernkonzept

Die ganze Architektur basiert auf WIEDERVERWENDBARKEIT:

1. **Character speichern** â†’ In DB
2. **Scene speichern** â†’ In DB
3. **Action speichern** â†’ In Story mit Sequenznummer
4. **SpÃ¤ter**: Gleiche Character in 5 verschiedenen Szenen verwenden
5. **SpÃ¤ter**: Story globale Assets Ã¤ndern (z.B. "alle Lights auf golden hour umschalten")

### Validierung: Forbidden Words Check

Nach jeder JSONâ†’Text Konvertierung prÃ¼ft das System automatisch auf:
- "masterpiece", "8K", "beautiful", etc.
- Falls gefunden: Warnung loggen, aber trotzdem zurÃ¼ckgeben

### Vision â†’ JSON

SpÃ¤ter (Phase 5) kannst du Gemini 1.5 Pro Vision nutzen, um:
1. User lÃ¤dt Foto hoch
2. Vision-Modell extrahiert: skin_tone, hair, clothing, setting, lighting
3. System generiert automatisch JSON
4. User kann editieren, dann abspeichern

---

## ðŸ“‹ TODO FÃœR MORGEN

- [x] `.env` mit GEMINI_API_KEY erstellen
- [x] `python gemini_integration.py` testen (sollte Prompt generieren)
- [x] `main.py` mit FastAPI-Endpoints schreiben
- [x] Lokalen Server starten (`uvicorn backend.main:app --reload`)
- [x] Frontend-Basis mit Next.js aufsetzen
- [x] Erstes Input-Form fÃ¼r "Generate Prompt" bauen

---

## ðŸš€ Phase 5: Frontend-Komponenten implementieren

Nachdem das GrundgerÃ¼st und die Design-Spezifikationen stehen, wird die erste Komponente implementiert.

### `PromptInputForm.tsx`

Diese Komponente ist das HerzstÃ¼ck der initialen BenutzeroberflÃ¤che. Sie befindet sich unter `frontend/components/ui/PromptInputForm.tsx` und realisiert das Eingabeformular, das in "Erste UI" beschrieben wurde. Sie nutzt die festgelegte Typografie und das Farbschema.

---

## ðŸ”— Ressourcen

- [Z-Image-Turbo Guide](../docs/gnph3X1n.txt) â€” Dein Regelwerk
- [Gemini API Docs](https://ai.google.dev/docs)
- [Pydantic Docs](https://docs.pydantic.dev/)
- [SQLAlchemy Docs](https://docs.sqlalchemy.org/)

---

**Du bist jetzt ready fÃ¼r Phase 2! ðŸš€**
